{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_dir = '/data/xenon/ehogenbi/psd_simulation/pickles/'\n",
    "data_dir   = '/data/xenon/ehogenbi/psd_simulation/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import blueice as bi\n",
    "from multihist import Histdd, Hist1d\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulse simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_photon_times(n_photons, t1, t3, fs, tts):\n",
    "    n_photons = int(n_photons)\n",
    "    times = np.zeros(n_photons)\n",
    "    n_singlets = np.random.binomial(n=n_photons, p=fs)\n",
    "    times += np.concatenate([\n",
    "        np.random.exponential(t1, n_singlets),\n",
    "        np.random.exponential(t3, n_photons - n_singlets)\n",
    "    ])\n",
    "    np.random.shuffle(times)\n",
    "    times += np.random.normal(0, tts, size=n_photons)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_likelihood_function(n_photons, t1, t3, fs, tts, hist_range, hist_bins, \n",
    "                              offset = None, plot=False, ):\n",
    "    if offset is None:\n",
    "        offset = 1 / n_photons\n",
    "    times = simulate_photon_times(n_photons, t1, t3, fs, tts)\n",
    "    vals, edges  = np.histogram(times, range = hist_range, bins = hist_bins, normed=True)\n",
    "    vals = np.max([vals, np.ones(len(vals)) * offset], axis=0)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    def f(x):\n",
    "        return np.interp(x, centers, vals)\n",
    "    def negloglikelihood(x):\n",
    "        return np.interp(x, centers, -np.log(vals))\n",
    "    \n",
    "    if plot:\n",
    "        x_plot = centers\n",
    "        plt.hist(times, range = hist_range, bins = hist_bins, normed=True, histtype = 'step')\n",
    "        plt.plot(centers, f(centers))\n",
    "        plt.yscale('log')\n",
    "    return negloglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def max_likelihood(s1, f):\n",
    "    '''\n",
    "    Minimize the value of the sum of the negative sum likelikhood given an array of times by shifting it \n",
    "    by a constant value in time.\n",
    "    Returns (sum neg log likelihood, shift)\n",
    "    '''\n",
    "    optres = scipy.optimize.minimize(lambda x: np.sum(f(s1 + x)), 0.)\n",
    "    shift = optres.x[0]\n",
    "    like  = optres.fun\n",
    "    return like, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood_ratio(s1, f_er, f_nr):\n",
    "    '''\n",
    "    Compute the log likelihood ratio of nr vs er.\n",
    "    (Note: it is minus cause there is already a log in max_likelihood!)\n",
    "    Low value is very NR-like.\n",
    "    '''\n",
    "    er_likelihood = max_likelihood(s1, f_er)\n",
    "    nr_likelihood = max_likelihood(s1, f_nr)\n",
    "    LR = nr_likelihood[0] - er_likelihood[0]\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_props(df, band):\n",
    "    df['band'] = band\n",
    "    # We do not divide by 1.15, already done in simulation\n",
    "    g1 =  0.1442 \n",
    "    g2 = 11.52 \n",
    "    df['e_rec'] = 13.7e-3 * ((df['cs1'] / g1) + (df['cs2'] / g2))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fit_values(E, band, X, field='low', tts= None):\n",
    "    '''\n",
    "    Extract the fit values for specific energies.\n",
    "    Returns a dict containing key values for that energy, interpolated!\n",
    "    If `return_tts`, will include tts as key.\n",
    "    '''\n",
    "    # Select fit values that are relevant\n",
    "    if field == 'high':\n",
    "        if band == 'er':\n",
    "            x = X[0]\n",
    "        elif band == 'nr':\n",
    "            x = X[1]\n",
    "    if field == 'low':\n",
    "        if band == 'er':\n",
    "            x = X[2]\n",
    "        elif band == 'nr':\n",
    "            x = X[3]\n",
    "    \n",
    "    es = x['e'] # energy list for the fit points\n",
    "    ret = {}\n",
    "    if not tts:\n",
    "        keys = ['t3', 'fs', 't1', 'tts']\n",
    "    else:\n",
    "        keys = ['t3', 'fs', 't1']\n",
    "    # Loop over keys and interpolate between values.    \n",
    "    for key in keys:\n",
    "        ret[key] = np.interp(E, es, x[key])\n",
    "    if tts:\n",
    "        ret['tts'] = tts\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lr(el, F_er, F_nr, X, field='low', tts=None):\n",
    "    '''\n",
    "    Simulate the shape of a pulse and return this value\n",
    "    '''\n",
    "    # We take the reconstructed energy: it must be valid for both ER and NR!\n",
    "    E = el['e_rec']\n",
    "    if E > 20:\n",
    "        print('Warning: energy %.1f out of range, replacing with 20 keV...' % (E))\n",
    "        E = 20.\n",
    "    \n",
    "    # We have bins of 1 keV (seems sufficient) so round and pull the likelihood function from the database\n",
    "    f_er = F_er[int(round(E))]\n",
    "    f_nr = F_nr[int(round(E))]\n",
    "    \n",
    "    # Now simulate the shape given the reconstructed energy\n",
    "    s1 = simulate_photon_times(el['s1_photons_detected'], **get_fit_values(E, el['band'], X, field=field, tts=tts))\n",
    "    # Get likelihood\n",
    "    lr = likelihood_ratio(s1, f_er, f_nr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_lr(df, F_er, F_nr, X, print_every = int(1e4), field='low', tts=None):\n",
    "    '''\n",
    "    Add the likelihood ratio to all events in dataframe.\n",
    "    '''\n",
    "    lrs = []\n",
    "    for i, el in df.iterrows():\n",
    "        lrs.append(get_lr(el, F_er, F_nr, X, field=field, tts=tts))\n",
    "        if i % print_every == 0:\n",
    "            print('%d of %d done, %.1f %%...' % (i, len(df), i / len(df) * 100))\n",
    "    df['lr'] = lrs\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produce_likelihood_library(start, stop, field, X, tts = None, n_photons = int(1e7),\n",
    "                               hist_range = (-50, 250), hist_bins = 1000):\n",
    "    '''\n",
    "    Produce the likelihoods per keV, for both ER and NR. \n",
    "    '''\n",
    "    num = (stop - start + 1)\n",
    "    F_er = np.array([\n",
    "        build_likelihood_function(int(1e7), hist_range = (-50, 250), hist_bins = 1000,\n",
    "                                **get_fit_values(energy, 'er', X, field = field, tts = tts)) \n",
    "                                for energy in np.linspace(start, stop, num)])\n",
    "    F_nr = np.array([\n",
    "        build_likelihood_function(int(1e7), hist_range = (-50, 250), hist_bins = 1000,\n",
    "                                **get_fit_values(energy, 'nr', X, field = field, tts = tts)) \n",
    "                                for energy in np.linspace(start, stop, num)])\n",
    "    return F_er, F_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get base model\n",
    "# If this is the first time you run it, it will take a few minutes\n",
    "print('Making base model....')\n",
    "from laidbax import base_model\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open(os.path.join(pickle_dir, 'X.pickle'), 'rb')) # Contains fit values\n",
    "Y = pickle.load(open(os.path.join(pickle_dir, 'X.pickle'), 'rb')) # Fits with tts fixed at 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_result(df, data_dir, base_name):\n",
    "    # Check current files in folder\n",
    "    fns = file_list(data_dir, base_name)\n",
    "    # This is the number that the file should get.\n",
    "    i = len(fns)\n",
    "    name = os.path.join(data_dir, base_name + '_%04d.pickle' % i)\n",
    "    pickle.dump(df[['cs1', 'cs2', 'lr']], open(name, 'wb'))\n",
    "    print('Dumped %d events to file %s. Done!' % (len(df), name))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_list(data_dir, base_name):\n",
    "    '''\n",
    "    List the files\n",
    "    '''\n",
    "    fns = os.listdir(data_dir)\n",
    "    fns = [os.path.join(data_dir, fn) for fn in fns if re.match(base_name + '_.....pickle', fn)]\n",
    "    fns = np.sort(fns)\n",
    "    return fns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
