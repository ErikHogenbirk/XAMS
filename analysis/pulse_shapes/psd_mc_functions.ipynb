{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1 pulse shape analysis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3.1\n",
    "  * Mock waveforms introduced\n",
    "\n",
    "Version 3.0\n",
    "  * per-s1 smearing to account for offset t0 and digitizer bin\n",
    "  * Sub-sample alignment using interpolation (does not influence results, but whatever)\n",
    "\n",
    "Version 2.5, December 4, 2017 (yes, still 2017)\n",
    "  * Bugfix for spe error (was only array for one channel...)\n",
    "  * Adding SPE errors photon by photon now (quad_adding default True)\n",
    "  * check_errors bugfix (no squares for chi2)\n",
    "  \n",
    "\n",
    "Version 2.4, December 1, 2017\n",
    "  * Re-introducted error_pct in sigma computation\n",
    "  * Added the data version of the spe error (though may be too large in current form...)\n",
    "  * bugfix for n steps in produce_settings_dicts\n",
    "  * Added plot_model_manual for already computed values :)\n",
    "  * Added check_errors for computed values\n",
    "\n",
    "Version 2.3, November 30, 2017\n",
    "  * Energy selection implemented (also in default, default is set to 5-20 keV to be roughly compatible)\n",
    "  * Dict splitting function\n",
    "  * Taking out s1_sample functionality\n",
    "  * Better S1 size array building in simulation (rather then just guessing)\n",
    "\n",
    "Version 2.2, November 29, 2017\n",
    "  * Splitting the errors in two parts, got rid of (minus, base, plus)\n",
    "  * Bugfix for sigma_stat (error was first added, then quadratic adding)\n",
    "  * Bugfix for sigma_syst (base model was computed twice)\n",
    "  * Took out the different modes for chi2\n",
    "  * Scheduled some depricated functions for removal\n",
    "  * Modified default parameters to high statistic ones: 250 bootstrap, 2e6 photons.\n",
    "  * stored_stat_value in parameters instead of globals\n",
    "  \n",
    "Version 2.1, November 27, 2017\n",
    "  * Major cleanup, removing some of the functionality....\n",
    "  * Including SPE error\n",
    "  * Added bootstrap error in value in memory\n",
    "  * Normalization of the average pulse within the time range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_dir = '/data/xenon/ehogenbi/pulsefit/pickles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "from scipy import interpolate\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import odeint\n",
    "from copy import deepcopy\n",
    "from multihist import Hist1d, Histdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Digitizer sample size\n",
    "dt = 2\n",
    "# time range. Note that cutting some of this out will also reduce the real data waveforms...\n",
    "# These are aligned by their center index!\n",
    "valid_t_range = (-100, 290)\n",
    "\n",
    "spe_ts = np.linspace(0, 639*2, 640) - 340 * 2\n",
    "# Valid time (because the waveform does not range the full time span)\n",
    "t_mask = (valid_t_range[0] <= spe_ts) & (spe_ts < valid_t_range[1])\n",
    "spe_ts = spe_ts[t_mask]\n",
    "spe_t_edges = np.concatenate([[spe_ts[0] - dt/2], spe_ts + dt/2])\n",
    "\n",
    "default_params = dict(\n",
    "    t1 = 3.1,    # Singlet lifetime, Nest 2014 p2\n",
    "    t3 = 24,     # Triplet lifetime, Nest 2014 p2\n",
    "    fs = 0.2,    # Singlet fraction\n",
    "    tts = 2.,     # Transit time spread.\n",
    "    f_r = 0.,\n",
    "    tr = 15,\n",
    "    fs_r = 0.2,\n",
    "    eta = 0.,\n",
    "    \n",
    "    s1_min = 0,\n",
    "    s1_max = 600,\n",
    "    e_min = 5,\n",
    "    e_max = 20,\n",
    "    dset = 'er',\n",
    "    aft = 0.28, # 0.28\n",
    "    n_photons = int(2e6),\n",
    "    t_min = -10.,\n",
    "    t_max = 125.,\n",
    "    error_offset  = 1e-4 , \n",
    "    error_pct = 0.,\n",
    "    neglect_statistical = False,\n",
    "    neglect_systematic = False,\n",
    "    s1_model = 'two_exp', # two_exp, recombination\n",
    "    bootstrap_trials = 250,\n",
    "    stored_stat = True,\n",
    "    stored_stat_value = None,\n",
    "    max_shift = 10,\n",
    ")\n",
    "\n",
    "def get_params(params):\n",
    "    '''\n",
    "    Returns full set of parameters, setting the values given in `params` and setting the values in \n",
    "    `default_params` if not set explicity.\n",
    "    '''\n",
    "    for k, v in default_params.items(): # key, value\n",
    "        params.setdefault(k, v)\n",
    "    if params['tts'] < 0:\n",
    "        params['tts'] = 1e-6\n",
    "    # Parameters that may not be smaller than zero\n",
    "    for par in ['fs', 'aft', 'f_r', 'fs_r', 'eta']:\n",
    "        if params[par] < 0:\n",
    "            params[par] = 0\n",
    "        elif params[par] >1:\n",
    "            params[par] =1\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PMT pulses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulse shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High resolution pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rebin(y, nsamples):\n",
    "#     if len(y) % nsamples != 0:\n",
    "#         raise ValueError('No es possibile')\n",
    "    nbins = int(len(y) / nsamples)\n",
    "    return np.average(np.reshape(y[:nbins * nsamples], (nbins, nsamples)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_fine = 0.1\n",
    "rebin_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the pickle file\n",
    "spe_ys_fine = []\n",
    "for ch in [0, 1]:\n",
    "    fn = os.path.join(pickle_dir, 'highrespulse_ch%d.pickle' % ch)\n",
    "    t_fine, ys = pickle.load(open(fn, 'rb'))\n",
    "    spe_ys_fine.append(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Errors\n",
    "with open(os.path.join(pickle_dir, 'highrespulse_error.pickle'), 'rb') as f:\n",
    "    _, spe_dys_fine = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rebin to speed up computation\n",
    "t_fine = rebin(t_fine, rebin_factor)\n",
    "spe_ys_fine = [rebin(ys, rebin_factor) for ys in spe_ys_fine]\n",
    "spe_dys_fine = rebin(spe_dys_fine, rebin_factor)\n",
    "dt_fine = dt_fine * rebin_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limiting the time range\n",
    "spe_ts_fine = t_fine\n",
    "t_mask = (valid_t_range[0] <= spe_ts_fine) & (spe_ts_fine < valid_t_range[1])\n",
    "spe_ts_fine = spe_ts_fine[t_mask]\n",
    "spe_t_edges_fine = np.concatenate([[spe_ts_fine[0] - dt_fine/2], spe_ts_fine + dt_fine/2])\n",
    "spe_ys_fine = [ys[t_mask] for ys in spe_ys_fine]\n",
    "spe_dys_fine = spe_dys_fine[t_mask]\n",
    "\n",
    "# Normalization\n",
    "spe_dys_fine = spe_dys_fine / (np.average(np.sum(spe_ys_fine, axis = 1))) * dt_fine / dt\n",
    "spe_ys_fine = [ys / np.sum(ys) * dt_fine / dt for ys in spe_ys_fine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_samples(y, nsamples):\n",
    "    '''\n",
    "    Shift samples to the right (negative means left)\n",
    "    '''\n",
    "    if nsamples == 0:\n",
    "        return y\n",
    "    elif nsamples > 0:\n",
    "        return np.concatenate([np.zeros(nsamples), y[:-nsamples]])\n",
    "    elif nsamples < 0:\n",
    "        return np.concatenate([y[ - nsamples:], np.zeros(-nsamples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gain_params = []\n",
    "for ch, fn in enumerate(['170323_103732', '170323_104831']):\n",
    "    with open(os.path.join(pickle_dir, '%s_ch%d_function.pickle' % (fn, ch)) , 'rb') as infile:\n",
    "        _norm, _popt, _perr = pickle.load(infile)\n",
    "        gain_params.append(np.concatenate([np.array([_norm]), _popt, _perr]))\n",
    "gain_params = np.array(gain_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def area_sample(n_values, gain_params, channel):\n",
    "    norm, mu, sigma, _, _ = gain_params[channel]\n",
    "    lower, upper = (0., 3.)\n",
    "    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "    return X.rvs(n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaus_trunc(x, mu, sigma):\n",
    "    return (x > 0) * np.exp( - (x - mu)**2 / (2 * sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombination model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_eta(t, tr, eta):\n",
    "    params = [tr, eta]\n",
    "    y0 = [(1- eta), 1]\n",
    "    # print(eta)\n",
    "    \n",
    "    # Define the differential equations to solve\n",
    "    def f(y, t, params):\n",
    "        n_minus, n_plus = y\n",
    "        tau, eta  = params\n",
    "        alpha = 1/tau\n",
    "        derivs = [- alpha * n_minus * n_plus, - alpha * n_minus * n_plus]\n",
    "        return derivs\n",
    "    \n",
    "    psoln = odeint(f, y0, t, args=(params,))\n",
    "    # Return only the electron number...\n",
    "    return psoln\n",
    "\n",
    "def n_product(t, tr, eta):\n",
    "    psoln = n_eta(t, tr, eta)\n",
    "    n_e = psoln[:, 0]\n",
    "    n_holes = psoln[:, 1]\n",
    "    return 1/ tr * n_e * n_holes\n",
    "\n",
    "\n",
    "def Ir3(t, tau, tr, eta, tmax, nsteps):\n",
    "    t_fine = np.linspace(0, tmax, nsteps)\n",
    "    pdf = np.exp(-t_fine/tau) / tau * np.cumsum(n_product(t_fine, tr, eta) * np.exp(t_fine/tau))\n",
    "    return np.interp(t, t_fine, pdf)\n",
    "\n",
    "def Ir3_cdf(t, tau, tr, eta, tmax, nsteps):\n",
    "    pdf = Ir3(t, tau, tr, eta, tmax, nsteps)\n",
    "    return np.cumsum(pdf) / sum(pdf)\n",
    "\n",
    "def Ir3_cdf_inv(t, tau, tr, eta, tmax, nsteps):\n",
    "    cdf = Ir3_cdf(t, tau, tr, eta, tmax, nsteps)\n",
    "    # 0 in the cdf means at time zero \n",
    "    return interpolate.interp1d(cdf, t, fill_value=(0, np.inf), bounds_error=False)\n",
    "\n",
    "def simulate_recombination_times2(nphotons, tau, tr, eta, tmax, nsteps):\n",
    "    t = np.linspace(0, tmax, nsteps)\n",
    "    return Ir3_cdf_inv(t, tau, tr, eta, tmax, nsteps)(np.random.rand(nphotons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "def split_s1_groups(x, n_x, areas, channels, **params):\n",
    "    \"\"\"Splits x into groups with uniform (s1_min, s1_max) elements, then return matrix of histograms per group.\n",
    "    Returns: integer array (n_x, n_groups)\n",
    "    n_x: number of possible values in x. Assumed to be from 0 ... n_x - 1\n",
    "    s1_min: minimum S1 number of hits\n",
    "    s1_max: maximum S1 number of hits\n",
    "    \"\"\"\n",
    "    params = get_params(params) \n",
    "    # Obtain the distribution that we wish to sample from\n",
    "    s1s_data   = xams_data[params['dset']]['s1']\n",
    "    e_data = xams_data[params['dset']]['e_ces']\n",
    "    s1s_data = s1s_data[(s1s_data >= params['s1_min']) & (s1s_data < params['s1_max']) & \n",
    "                            (e_data >= params['e_min']) & (e_data < params['e_max'])]\n",
    "    \n",
    "    # Build S1 size array until it is large enough\n",
    "    s1_block_size = int(1000)\n",
    "    margin = 1.1\n",
    "    pe_per_s1 = np.array([])\n",
    "    while np.sum(pe_per_s1) < margin * np.sum(areas):\n",
    "        # Add S1 sizes per block and check if large enough\n",
    "        pe_per_s1  = np.concatenate([pe_per_s1, np.random.choice(s1s_data, size=s1_block_size)])\n",
    "    # This is the length of the array, should be an overestimate of the number of S1s\n",
    "    n_s1_est = len(pe_per_s1)\n",
    "    # Shift per S1\n",
    "    # This shifts each S1 to a random 0.2 ns wide bin, so that we do not align on the zero point all the time\n",
    "    # Basically, this parameter is the time between true interaction and a digitizer edge\n",
    "    if params['max_shift'] != 0:\n",
    "        shifts_per_s1 = np.random.randint(0, params['max_shift'], size = n_s1_est)\n",
    "    else:\n",
    "        shifts_per_s1 = np.zeros(n_s1_est, dtype=int)\n",
    "    \n",
    "    # These are two arrays for the two channels\n",
    "    # i.e. these will later yield the top and bottom waveform\n",
    "    result0 = np.zeros((n_x, n_s1_est), dtype=float)\n",
    "    result1 = np.zeros((n_x, n_s1_est), dtype=float)\n",
    "    s1_i = _split_s1_groups(x, pe_per_s1, result0, result1, areas, channels, shifts_per_s1)\n",
    "    return result0[:,:s1_i - 1], result1[:,:s1_i - 1]\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _split_s1_groups(x, pe_per_s1, result0, result1, areas, channels, shifts_per_s1):\n",
    "    # One of these days, I'm going to cut you into little pieces\n",
    "    # x is time index array\n",
    "    s1_i = 0\n",
    "    for photon_i, (i, ch) in enumerate(zip(x, channels)):\n",
    "        if pe_per_s1[s1_i] < 0:\n",
    "            s1_i += 1\n",
    "            continue\n",
    "        if ch == 0:\n",
    "            result0[i + shifts_per_s1[s1_i], s1_i] += areas[photon_i]\n",
    "        if ch == 1:\n",
    "            result1[i + shifts_per_s1[s1_i], s1_i] += areas[photon_i]\n",
    "        pe_per_s1[s1_i] -= areas[photon_i]\n",
    "    return s1_i \n",
    "\n",
    "\n",
    "def shift(x, n):\n",
    "    \"\"\"Shift the array x n samples to the right, adding zeros to the left.\"\"\"\n",
    "    if n > 0:\n",
    "        return np.pad(x, (n, 0), mode='constant')[:len(x)]\n",
    "    else:\n",
    "        return np.pad(x, (0, -n), mode='constant')[-len(x):]\n",
    "\n",
    "\n",
    "\n",
    "def simulate_s1_pulse(**params):\n",
    "    # n_photons=int(2e5), \n",
    "    \"\"\"Return (wv_matrix, time_matrix, t_shift vector) for simulated S1s, consisting of n_photons in total\n",
    "    \"\"\"\n",
    "    params = get_params(params)\n",
    "    n_photons = params['n_photons']\n",
    "\n",
    "    ##\n",
    "    # Make matrix (n_samples, n_waveforms) of pulse waveforms with various shifts\n",
    "    ##\n",
    "    wv_matrix_list = []\n",
    "    for ch in [0, 1]:\n",
    "        # This is a matrix filled with waveforms, ordered by their SHIFT.\n",
    "        # So, these are all just model waveforms and will be selected later\n",
    "        y = spe_ys_fine[ch]\n",
    "        i_noshift = np.searchsorted(spe_t_edges_fine, [0])[0]    # Index corresponding to no shift in the waveform\n",
    "        wv_matrix = np.vstack([rebin(shift_samples(y, i - i_noshift), int(dt / dt_fine))\n",
    "                           for i in range(len(spe_ts_fine))]).T \n",
    "        wv_matrix_list.append(wv_matrix)\n",
    "    \n",
    "    # Include SPE errors\n",
    "    # Assumed the same for both channels...\n",
    "    y = spe_dys_fine\n",
    "    i_noshift = np.searchsorted(spe_t_edges_fine, [0])[0]    # Index corresponding to no shift in the waveform\n",
    "    wv_err_matrix = np.vstack([rebin(shift_samples(y, i - i_noshift), int(dt / dt_fine))\n",
    "                           for i in range(len(spe_ts_fine))]).T \n",
    "    \n",
    "    ##\n",
    "    # Simulate S1 pulse times, convert to index\n",
    "    ##\n",
    "\n",
    "    # Channel selector\n",
    "    n_top = np.random.binomial(n=n_photons, p=params['aft']) # Number of photons happening in top array\n",
    "    # First all the top channels, then all the bottom channels (ch1)\n",
    "    channels = np.concatenate([np.zeros(n_top, dtype=int), np.ones(n_photons - n_top, dtype=int)])\n",
    "    areas = np.concatenate([area_sample(n_top, gain_params, channel=0), \n",
    "                            area_sample(n_photons - n_top, gain_params, channel=1)])\n",
    "    # Shuffle the two lists the exact same way\n",
    "    channels, areas = unison_shuffled_copies(channels, areas)\n",
    "    \n",
    "    # Time is distributed according to exponential distribution\n",
    "    # This is the TRUE time of all the photons generated, assuming time=0  is the time of the interaction\n",
    "    times = np.zeros(n_photons)\n",
    "\n",
    "    if params['s1_model'] == 'two_exp':\n",
    "        n_singlets = np.random.binomial(n=n_photons, p=params['fs']) # We randomly select if the photon came from a \n",
    "                                                                     # singlet or triplet decay\n",
    "        times += np.concatenate([\n",
    "            np.random.exponential(params['t1'], n_singlets),\n",
    "            np.random.exponential(params['t3'], n_photons - n_singlets)\n",
    "        ])\n",
    "    elif params['s1_model'] == 'recombination':\n",
    "        n_recombination = np.random.binomial(n=n_photons, p=params['f_r'])\n",
    "        n_recombination_singlets = np.random.binomial(n=n_recombination, p=params['fs_r'])\n",
    "        n_recombination_triplets = n_recombination - n_recombination_singlets\n",
    "        n_direct = n_photons - n_recombination\n",
    "        n_direct_singlets = np.random.binomial(n=n_direct, p=params['fs'])\n",
    "        n_direct_triplets = n_direct - n_direct_singlets\n",
    "        assert (n_recombination_singlets + n_recombination_triplets + n_direct_singlets + n_direct_triplets == n_photons)        \n",
    "        times += np.concatenate([\n",
    "            np.random.exponential(params['t1'], n_direct_singlets),\n",
    "            np.random.exponential(params['t3'], n_direct_triplets),\n",
    "            simulate_recombination_times2(n_recombination_singlets, params['t1'], params['tr'], params['eta'],\n",
    "                                          250, 1251), \n",
    "            simulate_recombination_times2(n_recombination_triplets, params['t3'], params['tr'], params['eta'],\n",
    "                                          250, 1251), \n",
    "        ])    \n",
    "    else:\n",
    "        raise ValueError('S1 model type not understood, got this: %s' % params['s1_model'])\n",
    "\n",
    "    # Since `times` is now sorted in (singlet, triplet), shuffle them\n",
    "    np.random.shuffle(times)\n",
    "    \n",
    "    # Here we start taking into account detector physics: the transit time spread (simulated as normal dist.)\n",
    "    times += np.random.normal(0, params['tts'], size=n_photons)\n",
    "    \n",
    "    # Find the bin that the photon would be in if it were sampled.\n",
    "    # Now, we delete all the photons that are outside of the bin range and re-match to the bin centers\n",
    "    # (Check the searchsorted documentation)\n",
    "    indices = np.searchsorted(spe_t_edges_fine, times)\n",
    "    indices = indices[~((indices == 0) | (indices > (len(spe_t_edges_fine) - params['max_shift'])))] - 1\n",
    "    # This is the new amount of photons simulated\n",
    "    if len(indices) < n_photons:\n",
    "        # print('Warning: I just threw away %d photons...' % (n_photons - len(indices)))\n",
    "        n_photons = len(indices)\n",
    "        \n",
    "    ##\n",
    "    # Build instruction matrix, simulate waveforms\n",
    "    ##\n",
    "    # So far, we've just been simulating a bunch of photons (very many).\n",
    "    # We are now going to split this into S1s: the split will be made at a random point between s1_min and s1_max.\n",
    "    # `index_matrix` is a matrix split into groups forming S1s. \n",
    "    # We've got two for the two channels    \n",
    "    index_matrix0, index_matrix1 = split_s1_groups(indices, len(spe_t_edges_fine) - 1, areas, channels,\n",
    "                                                   **params)\n",
    "\n",
    "    # Now, index_matrix[:, 0] contains a list of number of entries for the shift for each timestamp in bin\n",
    "    n_s1 = index_matrix0.shape[1]\n",
    "    \n",
    "    # Remember that wv_matrix is a matrix of waveforms, each element at position i of which is shifted i samples\n",
    "    s1_waveforms = np.dot(wv_matrix_list[0], index_matrix0) + np.dot(wv_matrix_list[1], index_matrix1)\n",
    "    \n",
    "    \n",
    "    # This should work as long as the photon density is so low that the \n",
    "    # probability of many photons per bin is low (or the photons)\n",
    "    # will not be added independently any more\n",
    "    s1_waveforms_error = np.sqrt(np.dot(wv_err_matrix**2, index_matrix0**2 + index_matrix1**2))\n",
    "\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    # Alignment based on 10p point\n",
    "    ##\n",
    "    time_matrix, t_shift = aligned_time_matrix(spe_ts, s1_waveforms)    \n",
    "    return s1_waveforms, s1_waveforms_error, time_matrix, t_shift\n",
    "\n",
    "def aligned_time_matrix(ts, wv_matrix, mode = '10p_interp'):\n",
    "    \"\"\"Return time matrix that would align waveforms im wv_matrix.\n",
    "    As it turns out, 10p mode yields the same as 10p_interp if you use 2 ns samples in the average wf. Hmm. \n",
    "    \"\"\"\n",
    "    n_s1 = wv_matrix.shape[1]\n",
    "    fraction_reached = np.cumsum(wv_matrix, axis=0) / np.sum(wv_matrix, axis=0)\n",
    "    \n",
    "    if mode == '10p':\n",
    "        # Get the sample where 10% is reached by taking the sample closest to the 10% point\n",
    "        distance_to_10p_point = np.abs(fraction_reached - 0.1)              \n",
    "        t_shift = ts[np.argmin(distance_to_10p_point, axis=0)]\n",
    "    elif mode == '10p_interp':\n",
    "        # Get the sample index corresponding to the sample just before the crossing\n",
    "        crossing_index = np.argmax(fraction_reached > 0.1, axis=0) - 1\n",
    "        # Get fractions before and after crossing (thanks numpy indexing, I want my two hours back)\n",
    "        f0 = fraction_reached[crossing_index, range(len(crossing_index))]\n",
    "        f1 = fraction_reached[crossing_index + 1, range(len(crossing_index))]\n",
    "        # Now add the time at that point and the interpolation\n",
    "        delta_t = 2.* (0.1 - f0) / (f1 - f0)\n",
    "        t_shift = ts[crossing_index] + delta_t       \n",
    "    time_matrix = np.repeat(ts, n_s1).reshape(wv_matrix.shape)\n",
    "    time_matrix -= t_shift[np.newaxis,:]\n",
    "    return time_matrix, t_shift\n",
    "\n",
    "def average_pulse(time_matrix, wv_matrix, **params):\n",
    "    \"\"\"Return average pulse, given time and waveform matrices\"\"\"\n",
    "    params = get_params(params)\n",
    "    h, _     = np.histogram(time_matrix, bins=spe_t_edges, weights=wv_matrix)\n",
    "    # normalize within time range\n",
    "    sel  = (spe_ts >= params['t_min']) & (spe_ts < params['t_max'])\n",
    "    h = h / np.sum(h[sel])\n",
    "    return h\n",
    "\n",
    "def average_pulse_and_error(time_matrix, wv_matrix, wv_err_matrix, **params):\n",
    "    \"\"\"Return average pulse and error, given time and waveform matrices\"\"\"\n",
    "    params = get_params(params)\n",
    "    h, _     = np.histogram(time_matrix, bins=spe_t_edges, weights=wv_matrix)\n",
    "    h_err, _ = np.histogram(time_matrix, bins=spe_t_edges, weights=wv_err_matrix)\n",
    "    sel  = (spe_ts >= params['t_min']) & (spe_ts < params['t_max'])\n",
    "    h_err = h_err / np.sum(h[sel])\n",
    "    h = h / np.sum(h[sel])\n",
    "    return h, h_err\n",
    "\n",
    "def s1_average_pulse_model(*args, **kwargs):\n",
    "    wv_matrix, wv_err_matrix, time_matrix, _ = simulate_s1_pulse(*args, **kwargs)\n",
    "    model, error = average_pulse_and_error(time_matrix, wv_matrix, wv_err_matrix,**kwargs)\n",
    "    return model, error\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    '''Stack overflow to the rescue'''\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "def s1_models_error(*args, shifts=None, mode='std', **kwargs):\n",
    "    '''\n",
    "    Gives the systematic error, split into two parts.\n",
    "    Returns `model`, `sigma_syst`, `spe_error`\n",
    "    '''\n",
    "    if kwargs.get('neglect_systematic', default_params['neglect_systematic']):\n",
    "        model, error = s1_average_pulse_model(*args, **kwargs)\n",
    "        return model, np.zeros(len(model)), error\n",
    "    \n",
    "    if shifts is None:\n",
    "        # Default uncertainty: in pulse model\n",
    "        shifts = dict(aft=0.12)\n",
    "    \n",
    "    # Allow specifying a single +- amplitude of variation\n",
    "    for p, shift_values in shifts.items():\n",
    "        if isinstance(shift_values, (float, int)):\n",
    "            shifts[p] = kwargs.get(p, default_params[p]) + np.array([-1, 0, 1]) * shift_values\n",
    "    \n",
    "\n",
    "    shift_pars = sorted(shifts.keys())\n",
    "    shift_values = [shifts[k] for k in shift_pars]\n",
    "    # shift_value_combs is a list of paramters that will be tried to compute the average pulse.\n",
    "    # Contains all combintations: (+, 0, -) for all the parameters. ((3n)^2 for n number of parameters.)\n",
    "    shift_value_combs = list(itertools.product(*shift_values))\n",
    "    noshift_comb = tuple([(kwargs.get(p, default_params[p])) for p, shift_values in shifts.items()])\n",
    "    noshift_index = int((len(shift_value_combs) -1) /2)\n",
    "    # Check if we have the right index\n",
    "    \n",
    "    for i, comb in enumerate(shift_value_combs):\n",
    "        if np.all([np.isclose(a,b) for a, b in zip(comb, noshift_comb)]):\n",
    "            noshift_index = i\n",
    "                \n",
    "    \n",
    "    alt_models = []\n",
    "    for vs in shift_value_combs:\n",
    "        kw = dict()\n",
    "        kw.update(kwargs)\n",
    "        for i, p in enumerate(shift_pars):\n",
    "            kw[p] = vs[i]        \n",
    "        alt_models.append(s1_average_pulse_model(*args, **kw))\n",
    "    # The alt_models list will now contain (model, error) for each parameter combination.\n",
    "    # Here, split them in twain\n",
    "    alt_errs = [alt_model[1] for alt_model in alt_models]\n",
    "    alt_models = [alt_model[0] for alt_model in alt_models]\n",
    "    \n",
    "    alt_models = np.vstack(alt_models)\n",
    "    spe_err = np.vstack(alt_errs)\n",
    "    model = alt_models[noshift_index]\n",
    "    \n",
    "    spe_err = alt_errs[noshift_index]\n",
    "    sigma_sys = np.sqrt(np.std(alt_models, axis=0)**2)\n",
    "    #minus = base_model - sigma_sys\n",
    "    #plus = base_model + sigma_sys\n",
    "\n",
    "    return model, sigma_sys, spe_err\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the S1 data for three (highfield) datasets: NR, ER and BG_NR. We store it in the form of a dict (keys: er, nr, bg_nr). Each dict item is an array containing the waveforms (per row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading XAMS data from pickles...\n"
     ]
    }
   ],
   "source": [
    "print('Reading XAMS data from pickles...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data in 23.14 seconds\n"
     ]
    }
   ],
   "source": [
    "before = time.time()\n",
    "xams_data = dict()\n",
    "xams_data['nr'], xams_data['er'], xams_data['bg_nr'] = pickle.load(open(os.path.join(pickle_dir, \n",
    "                                                                                     'highfield_dataframes.pickle'),'rb'))\n",
    "xams_data['er_0'] = pickle.load(open(os.path.join(pickle_dir, 'zerofield_dataframes_temp.pickle'), 'rb'))\n",
    "xams_data['nr_l'], xams_data['er_l'] = pickle.load(open(os.path.join(pickle_dir, 'lowfield_dataframes.pickle'), 'rb'))\n",
    "xams_s1s = dict()\n",
    "# Get pulse waveforms to matrix rather than object column\n",
    "# Also cut off a bit at the end to account for shorter waveform in high resolution pulse\n",
    "max_index = len(spe_ts)\n",
    "for k, d in xams_data.items():\n",
    "    xams_s1s[k] = np.array([x[:max_index] for x in d['s1_pulse']])\n",
    "    del d['s1_pulse']\n",
    "after = time.time()\n",
    "print('Read data in %.2f seconds' % (after - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now use 0.28 +- 0.12 as uncertainty of the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment, averaging, bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def real_s1_wv(**params):\n",
    "    \"\"\"Return average S1 waveform, number of S1s it was constructed from\"\"\"\n",
    "    params = get_params(params)\n",
    "    \n",
    "    areas    = xams_data[params['dset']]['s1'].values\n",
    "    energies = xams_data[params['dset']]['e_ces'].values\n",
    "    mask = ((params['s1_min'] < areas) & (areas < params['s1_max']) &\n",
    "            (energies >= params['e_min']) & (energies < params['e_max']))\n",
    "\n",
    "    n_data_s1s = mask.sum()\n",
    "    wvs = xams_s1s[params['dset']][mask].T\n",
    "    tmat, _ = aligned_time_matrix(spe_ts, wvs)\n",
    "    real_s1_avg =  average_pulse(tmat, wvs, **params)\n",
    "    \n",
    "    return real_s1_avg, n_data_s1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_s1_wv_sigma(**params):\n",
    "    \"\"\"Take data S1s, bootstrap sample, then check what the variance is\"\"\"\n",
    "    params = get_params(params)\n",
    "    if params['stored_stat']:\n",
    "        if params['stored_stat_value'] is not None:\n",
    "            return params['stored_stat_value']\n",
    "        else:\n",
    "            print('Warning: stored_stat not found, will revert to computing again...')\n",
    "    bootstrap_trials = params['bootstrap_trials']\n",
    "    time_matrix, wv_matrix = real_s1_wv_matrix(**params)\n",
    "    n_s1s = wv_matrix.shape[1]\n",
    "    waveform_templates = np.zeros((len(spe_ts), bootstrap_trials))\n",
    "\n",
    "    for i in range(bootstrap_trials):\n",
    "        new_indices = np.random.randint(n_s1s, size=n_s1s)\n",
    "\n",
    "        waveform_templates[:, i] = average_pulse(time_matrix[:, new_indices], \n",
    "                                                 wv_matrix[:, new_indices], **params)\n",
    "        \n",
    "    \n",
    "    return np.std(waveform_templates, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_s1_wv_matrix(**params):\n",
    "    \"\"\"Return the aligned time matrix and waveform matrix.\"\"\"\n",
    "    params = get_params(params)\n",
    "    \n",
    "    areas = xams_data[params['dset']]['s1'].values\n",
    "    mask = (params['s1_min'] < areas) & (areas < params['s1_max'])\n",
    "\n",
    "    n_data_s1s = mask.sum()\n",
    "    wvs = xams_s1s[params['dset']][mask].T\n",
    "    tmat, _ = aligned_time_matrix(spe_ts, wvs)\n",
    "        \n",
    "    return tmat, wvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-data comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigma and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def residuals(ydata, model, syst_err, spe_err, **params):\n",
    "    params = get_params(params)\n",
    "    sigma = get_sigma(model, syst_err, spe_err, **params)\n",
    "    if 0. in sigma:\n",
    "        zero_positions = np.where(sigma == 0)\n",
    "        print('Warning: found zero in error array at positions: ', zero_positions)\n",
    "        print('Replacing with infinite error instead...')\n",
    "        for pos in zero_positions:\n",
    "            sigma[pos] = np.inf\n",
    "    return (ydata - model) / sigma\n",
    "\n",
    "def get_sigma(model, syst_err, spe_err, **params):\n",
    "    '''\n",
    "    Combine the syst. errors with stat error.\n",
    "    '''\n",
    "    params = get_params(params)\n",
    "    if params['neglect_statistical']:\n",
    "        print('Neglecting statistical error...')\n",
    "        return np.sqrt(syst_err**2 + spe_err**2)\n",
    "    sigma_stat = real_s1_wv_sigma(**params)\n",
    "    sigma = np.sqrt(syst_err**2 + spe_err**2 + sigma_stat**2 + \n",
    "                    params['error_offset']**2) #  + (model * params['error_pct'])**2\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparison_plot(ydata, model, syst_err, spe_err, log=False, **params):\n",
    "    params = get_params(params)\n",
    "    sigmas = get_sigma(model, syst_err, spe_err, **params)\n",
    "\n",
    "    # large subplot\n",
    "    ax2 = plt.subplot2grid((3,1), (2,0))\n",
    "    ax1 = plt.subplot2grid((3,1), (0,0), rowspan=2, sharex=ax2)\n",
    "\n",
    "    #f, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    plt.sca(ax1)\n",
    "    # plt.fill_between(spe_ts, minus, plus, alpha=0.5, linewidth=0, step='mid')\n",
    "    plt.fill_between(spe_ts, model - sigmas, model + sigmas,\n",
    "                     alpha=0.5, linewidth=0, step='mid')\n",
    "    plt.plot(spe_ts, model, linestyle='steps-mid', label='Model')\n",
    "    plt.plot(spe_ts, ydata, marker='.', linestyle='', markersize=3, c='k', label='Observed')\n",
    "\n",
    "    plt.grid(alpha=0.1, linestyle='-', which='both')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.ylabel(\"Fraction of amplitude\")\n",
    "    plt.axhline(0, c='k', alpha=0.5)\n",
    "    leg = plt.legend(loc='upper right', numpoints=1)\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(1e-5, 1e-1)\n",
    "    else:\n",
    "        plt.ylim(0, None)\n",
    "\n",
    "    # Add residuals\n",
    "    plt.sca(ax2)\n",
    "    plt.subplot2grid((3,1), (2,0), sharex=ax1)\n",
    "    plt.xlim(params['t_min'], params['t_max'])\n",
    "\n",
    "    res = (ydata - model) / sigmas # residuals(ydata, minus, base, plus, **params)\n",
    "    \n",
    "    plt.plot(spe_ts, res,\n",
    "             linestyle='--', marker='x', c='k', markersize=3)\n",
    "    # plt.ylim(-3, 3)\n",
    "    plt.grid(which='both', linestyle='-', alpha=0.1)\n",
    "    plt.axhline(0, c='k', alpha=0.5)\n",
    "\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.xlabel(\"Time since alignment point\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(0,0,1,1,0,0)\n",
    "\n",
    "def comparison_plot_2(ydata, model, syst_err, spe_err, plot_residual = True, **params):\n",
    "    params = get_params(params)\n",
    "    sigmas = get_sigma(model, syst_err, spe_err, **params)\n",
    "    res = (ydata - model) / sigmas \n",
    "\n",
    "    plt.fill_between(spe_ts, model - sigmas, model + sigmas,\n",
    "                     alpha=0.5, linewidth=0, step='mid')\n",
    "    plt.plot(spe_ts, model, linestyle='steps-mid', label='Model')\n",
    "    plt.plot(spe_ts, ydata, marker='.', linestyle='', markersize=3, c='k', label='Observed')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(2e-5, 1e-1)\n",
    "    plt.ylabel(\"Fraction of amplitude\")\n",
    "    plt.xlabel('Time (ns)')\n",
    "    for _l in (params['t_min'], params['t_max']):\n",
    "        plt.axvline(_l, ls='dotted', color='black')\n",
    "    if plot_residual:\n",
    "        plt.twinx()\n",
    "        plt.plot(spe_ts, np.abs(res), color='red')\n",
    "        plt.ylabel('Residual / error')\n",
    "        plt.ylim(0)\n",
    "    plt.xlim(params['t_min'] - 20, params['t_max'] + 50)\n",
    "    \n",
    "    res = res[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])]\n",
    "    chi2 = sum(res**2) / len(spe_ts[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])])\n",
    "    print('chi2 = %f' % chi2)\n",
    "    \n",
    "def plot_model(plot_type = 0, figname = None, plot_residual = True, verbose=True, **params):\n",
    "    params = get_params(params)\n",
    "    if verbose:\n",
    "        print(params)\n",
    "    ydata, _ = real_s1_wv(**params)\n",
    "    model, syst_err, spe_err = s1_models_error(**params)\n",
    "    if plot_type == 0 or plot_type == 1:\n",
    "        comparison_plot(ydata, model, syst_err, spe_err, **params)\n",
    "        if figname is not None:\n",
    "            plt.savefig('figs/' + figname + '_1.png', bbox_inches = 'tight', dpi = 400)\n",
    "        plt.show()\n",
    "    if plot_type == 0 or plot_type == 2:\n",
    "        comparison_plot_2(ydata, model, syst_err, spe_err, plot_residual = plot_residual, **params)\n",
    "        if figname is not None:\n",
    "            plt.savefig('figs/' + figname + '_2.png', bbox_inches = 'tight', dpi = 400)\n",
    "\n",
    "        plt.show()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gof(verbose=True, **params):\n",
    "    '''\n",
    "    Get the value to minimize given the parameters\n",
    "    '''\n",
    "    params = get_params(params)\n",
    "    # Do not allow unphysical values\n",
    "    if params['t1'] < 0 or params['t3'] < 0 or not (0 <= params['fs'] <= 1):\n",
    "        result = float('inf')\n",
    "    else:\n",
    "        ydata, _ = real_s1_wv(**params)\n",
    "        model, syst_err, spe_err = s1_models_error(**params)\n",
    "        res = residuals(ydata, model, syst_err, spe_err, **params)\n",
    "        assert len(res) == len(spe_ts)\n",
    "        res = res[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])]\n",
    "        # Computed chi2 over NDF\n",
    "        result = 1/len(res) * np.sum(res**2)\n",
    "    if verbose:\n",
    "        print('gof={gof}, fs={fs}, t1={t1}, t3={t3}, tts={tts}'.format(gof=result, **params))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "def minimize_it(param_names, starting_values, direc, **params):\n",
    "    optresult = optimize.minimize(\n",
    "        lambda x: gof(**merge_two_dicts(params, {par : x[i] for i, par in enumerate(param_names)})),\n",
    "        starting_values,\n",
    "        options=dict(maxfev=1000, direc=direc),\n",
    "        method='Powell',\n",
    "    )\n",
    "    return optresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automatic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_settings_dicts(scan_params, lower_bounds, upper_bounds, step_sizes, block_size=int(1e6), verbose=True, **p):\n",
    "    '''\n",
    "    Build a number of settings dicts in an array. These settings scan over the bounds values.\n",
    "    '''\n",
    "    x = []\n",
    "    nsteps = [int(np.round((u - l)/s)) + 1 for u, l, s in zip(upper_bounds, lower_bounds, step_sizes)]\n",
    "    for param, l, u, ns in zip(scan_params, lower_bounds, upper_bounds, nsteps):\n",
    "        x.append(np.linspace(l, u, ns))\n",
    "    n_sim  = int(np.product(nsteps))\n",
    "    x = np.meshgrid(*x)\n",
    "    par_vals = [_x.flatten() for _x in x]\n",
    "    dicts = []\n",
    "    for i in range(n_sim):\n",
    "        for j, par in enumerate(scan_params):\n",
    "            p[par] = par_vals[j][i]\n",
    "        dicts.append(deepcopy(p))\n",
    "    dicts = np.array(dicts)\n",
    "    if verbose: print('Loaded %d settings.' % len(dicts))\n",
    "    if len(dicts) <= block_size:\n",
    "        return dicts\n",
    "    else:\n",
    "        n_blocks = int(len(dicts) / block_size)\n",
    "        return np.array_split(dicts, n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_settings(dicts):\n",
    "    for p in dicts:\n",
    "        p['chi2'] = gof(**p)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_settings_pickles(x, dirname, base_name):\n",
    "    fns = []\n",
    "    for i, _x in enumerate(x):\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        fn = os.path.join(dirname, base_name + '_%03d.pickle' % i)\n",
    "        fns.append(fn)\n",
    "        with open(fn, 'wb') as f:\n",
    "            pickle.dump(_x, f)\n",
    "    print('Dumping done.')\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_pickle_read(fn):\n",
    "    with open(fn, 'rb') as f:\n",
    "        dicts = pickle.load(f)\n",
    "    return dicts\n",
    "\n",
    "def single_pickle_dump(fn, dicts):\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(dicts, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_settings(dict_list, nsamples):\n",
    "    n_lists = int(np.floor(len(dict_list) / nsamples))\n",
    "    return np.array_split(dict_list, n_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are meant to analyze the data post-processing, assuming that all the relevant information is in the procesed data files (i.e. dicts). This means nothing needs to be recomputed, yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model_manual(plot_type = 0, figname = None, **params):\n",
    "    '''\n",
    "    Plot the model assuming that all needed parameters are in `params` (such as syst error etc.)\n",
    "    '''\n",
    "    params = get_params(params)\n",
    "    ydata, model, syst_err, spe_err = (params['ydata'], params['model'], params['syst_err'], params['spe_err'])\n",
    "    if plot_type == 0 or plot_type == 1:\n",
    "        comparison_plot(**params)\n",
    "        if figname is not None:\n",
    "            plt.savefig('figs/' + figname + '_1.png', bbox_inches = 'tight', dpi = 400)\n",
    "        plt.show()\n",
    "    if plot_type == 0 or plot_type == 2:\n",
    "        comparison_plot_2(**params)\n",
    "        if figname is not None:\n",
    "            plt.savefig('figs/' + figname + '_2.png', bbox_inches = 'tight', dpi = 400)\n",
    "\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_errors(**p):\n",
    "    \"\"\"\n",
    "    Plot the errors by type as function of time and check where the sigma contribution lies.\n",
    "    Nice for nailing down the error definitions.\n",
    "    \"\"\"\n",
    "    p = get_params(p)\n",
    "    for key in ['spe_err', 'stored_stat_value', 'syst_err']:\n",
    "        plt.plot(spe_ts, p[key], label= key)\n",
    "    plt.plot(spe_ts, np.ones(len(spe_ts)) * p['error_offset'], label='error_offset')\n",
    "    sigmas = get_sigma(**p)\n",
    "    plt.plot(spe_ts, sigmas, label='Total error')\n",
    "    plt.legend(loc=(1.05,0))\n",
    "\n",
    "    for t in [p['t_min'], p['t_max']]:\n",
    "        plt.axvline(t, ls='--', lw=1, color='black')\n",
    "    plt.xlim(p['t_min'] -10, p['t_max'] + 25)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    deltas = np.abs(p['model'] - p['ydata'])\n",
    "    plt.plot(spe_ts, deltas)\n",
    "    plt.plot(spe_ts, sigmas)\n",
    "    peak = (spe_ts >= -10) & (spe_ts < 30)\n",
    "    tail = (spe_ts >= 30) & (spe_ts < 125)\n",
    "    inrange = (spe_ts >= -10) & (spe_ts < 125)\n",
    "    print('Chi2 in peak: (-10 to 30 ns): %.2f' % (np.average(deltas[peak]**2 / sigmas[peak]**2)))\n",
    "    print('Chi2 in tail: (30 to 125 ns): %.2f' % (np.average(deltas[tail]**2 / sigmas[tail]**2)))\n",
    "    print('Chi2 total: %.2f' % (np.average(deltas[inrange]**2 / sigmas[inrange]**2)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gof_manual(ydata, model, syst_err, spe_err, verbose=True, **params):\n",
    "    params = get_params(params)\n",
    "    res = residuals(ydata, model, syst_err, spe_err, **params)\n",
    "    assert len(res) == len(spe_ts)\n",
    "    res = res[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])]\n",
    "    # Computed chi2 over NDF\n",
    "    result = 1/len(res) * np.sum(res**2)\n",
    "    if verbose: print('gof={gof}, fs={fs}, t1={t1}, t3={t3}, tts={tts}'.format(gof=result, **params))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock waveforms production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New jargon warning: 'mock data' is simulated data that is meant to simulate the actual amount of waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mock_s1_wv_sigma(time_matrix, wv_matrix, **params):\n",
    "    \"\"\"Take data S1s, bootstrap sample, then check what the variance is\"\"\"\n",
    "    params = get_params(params)\n",
    "    bootstrap_trials = params['bootstrap_trials']\n",
    "    n_s1s = wv_matrix.shape[1]\n",
    "    waveform_templates = np.zeros((len(spe_ts), bootstrap_trials))\n",
    "\n",
    "    for i in range(bootstrap_trials):\n",
    "        new_indices = np.random.randint(n_s1s, size=n_s1s)\n",
    "\n",
    "        waveform_templates[:, i] = average_pulse(time_matrix[:, new_indices], \n",
    "                                                 wv_matrix[:, new_indices], **params)\n",
    "        \n",
    "    return np.std(waveform_templates, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mock_data(verbose=True, return_stored = True, **params):\n",
    "    '''\n",
    "    Mock data is simulated data with just the right statistics given the data.\n",
    "    Arguments: parameters specifying the dataset (dset, e_min, e_max), plus the true values (i.e. the fitted values).\n",
    "    '''\n",
    "#     params = get_params(params)\n",
    "    d = xams_data[params['dset']]\n",
    "    d = d[(d['e_ces'] >= params['e_min']) & (d['e_ces'] < params['e_max']) ]\n",
    "    n_s1s = len(d)\n",
    "    if verbose:\n",
    "        print(\"Selected data from %s, energy from %d to %d\" % (params['dset'], params['e_min'], params['e_max']))\n",
    "        print(\"Selected %d waveforms\" % (n_s1s))\n",
    "    \n",
    "    # Take a generous sample of photons to eneter into the simulation\n",
    "    n_photons_guess = int(1.5 * sum(d['cs1']))\n",
    "    s1_waveforms, s1_waveforms_error, time_matrix, t_shift  = simulate_s1_pulse(n_photons = n_photons_guess, **params)\n",
    "    # Cut off the excess waveforms to exactly match stats\n",
    "    if len(t_shift) < n_s1s:\n",
    "        raise ValueError(\"Number of S1s incorrect, how is possible?\")\n",
    "    s1_waveforms = s1_waveforms[:, :n_s1s]\n",
    "    s1_waveforms_error = s1_waveforms_error[:, :n_s1s]\n",
    "    t_shift = t_shift[:n_s1s]\n",
    "    time_matrix = time_matrix[:, :n_s1s]\n",
    "    avg = average_pulse(time_matrix, s1_waveforms)\n",
    "    if return_stored:\n",
    "        stored_stat = mock_s1_wv_sigma(time_matrix, s1_waveforms)\n",
    "        return avg, stored_stat\n",
    "    else:\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
