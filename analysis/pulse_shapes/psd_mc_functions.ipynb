{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1 pulse shape analysis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2.2, November 29, 2017\n",
    "  * Splitting the errors in two parts, got rid of (minus, base, plus)\n",
    "  * Bugfix for sigma_stat (error was first added, then quadratic adding)\n",
    "  * Bugfix for sigma_syst (base model was computed twice)\n",
    "  * Took out the different modes for chi2\n",
    "  * Scheduled some depricated functions for removal\n",
    "  * Modified default parameters to high statistic ones: 250 bootstrap, 2e6 photons.\n",
    "  * stored_stat_value in parameters instead of globals\n",
    "  \n",
    "Version 2.1, November 27, 2017\n",
    "  * Major cleanup, removing some of the functionality....\n",
    "  * Including SPE error\n",
    "  * Added bootstrap error in value in memory\n",
    "  * Normalization of the average pulse within the time range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_dir = '/data/xenon/ehogenbi/pulsefit/pickles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import odeint\n",
    "from copy import deepcopy\n",
    "from multihist import Hist1d, Histdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Digitizer sample size\n",
    "dt = 2\n",
    "# time range. Note that cutting some of this out will also reduce the real data waveforms...\n",
    "# These are aligned by their center index!\n",
    "valid_t_range = (-100, 290)\n",
    "\n",
    "spe_ts = np.linspace(0, 639*2, 640) - 340 * 2\n",
    "# Valid time (because the waveform does not range the full time span)\n",
    "t_mask = (valid_t_range[0] <= spe_ts) & (spe_ts < valid_t_range[1])\n",
    "spe_ts = spe_ts[t_mask]\n",
    "spe_t_edges = np.concatenate([[spe_ts[0] - dt/2], spe_ts + dt/2])\n",
    "\n",
    "default_params = dict(\n",
    "    t1 = 3.1,    # Singlet lifetime, Nest 2014 p2\n",
    "    t3 = 24,     # Triplet lifetime, Nest 2014 p2\n",
    "    fs = 0.2,    # Singlet fraction\n",
    "    tts = 2.,     # Transit time spread.\n",
    "    f_r = 0.,\n",
    "    tr = 15,\n",
    "    fs_r = 0.2,\n",
    "    eta = 0.,\n",
    "    \n",
    "    s1_min=30,\n",
    "    s1_max=100,\n",
    "    dset = 'er',\n",
    "    aft = 0.28, # 0.28\n",
    "    n_photons = int(2e6),\n",
    "    t_min = -10.,\n",
    "    t_max = 125.,\n",
    "    s1_sample = 'data', # 'uniform'\n",
    "    error_offset  = 1e-4 , \n",
    "    error_pct = 0.,\n",
    "    neglect_statistical = False,\n",
    "    neglect_systematic = False,\n",
    "    s1_model = 'two_exp', # two_exp, recombination\n",
    "    bootstrap_trials = 250,\n",
    "    stored_stat = True,\n",
    "    stored_stat_value = None,\n",
    ")\n",
    "\n",
    "def get_params(params):\n",
    "    '''\n",
    "    Returns full set of parameters, setting the values given in `params` and setting the values in \n",
    "    `default_params` if not set explicity.\n",
    "    '''\n",
    "    for k, v in default_params.items(): # key, value\n",
    "        params.setdefault(k, v)\n",
    "    if params['tts'] < 0:\n",
    "        params['tts'] = 1e-6\n",
    "    # Parameters that may not be smaller than zero\n",
    "    for par in ['fs', 'aft', 'f_r', 'fs_r', 'eta']:\n",
    "        if params[par] < 0:\n",
    "            params[par] = 0\n",
    "        elif params[par] >1:\n",
    "            params[par] =1\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PMT pulses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulse shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High resolution pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rebin(y, nsamples):\n",
    "#     if len(y) % nsamples != 0:\n",
    "#         raise ValueError('No es possibile')\n",
    "    nbins = int(len(y) / nsamples)\n",
    "    return np.average(np.reshape(y[:nbins * nsamples], (nbins, nsamples)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_fine = 0.1\n",
    "rebin_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the pickle file\n",
    "spe_ys_fine = []\n",
    "for ch in [0, 1]:\n",
    "    fn = os.path.join(pickle_dir, 'highrespulse_ch%d.pickle' % ch)\n",
    "    t_fine, ys = pickle.load(open(fn, 'rb'))\n",
    "    spe_ys_fine.append(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rebin to speed up computation\n",
    "t_fine = rebin(t_fine, rebin_factor)\n",
    "spe_ys_fine = [rebin(ys, rebin_factor) for ys in spe_ys_fine]\n",
    "dt_fine = dt_fine * rebin_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spe_ts_fine = t_fine\n",
    "t_mask = (valid_t_range[0] <= spe_ts_fine) & (spe_ts_fine < valid_t_range[1])\n",
    "spe_ts_fine = spe_ts_fine[t_mask]\n",
    "spe_t_edges_fine = np.concatenate([[spe_ts_fine[0] - dt_fine/2], spe_ts_fine + dt_fine/2])\n",
    "spe_ys_fine = [ys[t_mask] for ys in spe_ys_fine]\n",
    "spe_ys_fine = [ys / np.sum(ys) * dt_fine / dt for ys in spe_ys_fine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_samples(y, nsamples):\n",
    "    '''\n",
    "    Shift samples to the right (negative means left)\n",
    "    '''\n",
    "    if nsamples == 0:\n",
    "        return y\n",
    "    elif nsamples > 0:\n",
    "        return np.concatenate([np.zeros(nsamples), y[:-nsamples]])\n",
    "    elif nsamples < 0:\n",
    "        return np.concatenate([y[ - nsamples:], np.zeros(-nsamples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gain_params = []\n",
    "for ch, fn in enumerate(['170323_103732', '170323_104831']):\n",
    "    with open(os.path.join(pickle_dir, '%s_ch%d_function.pickle' % (fn, ch)) , 'rb') as infile:\n",
    "        _norm, _popt, _perr = pickle.load(infile)\n",
    "        gain_params.append(np.concatenate([np.array([_norm]), _popt, _perr]))\n",
    "gain_params = np.array(gain_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def area_sample(n_values, gain_params, channel):\n",
    "    norm, mu, sigma, _, _ = gain_params[channel]\n",
    "    lower, upper = (0., 3.)\n",
    "    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "    return X.rvs(n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaus_trunc(x, mu, sigma):\n",
    "    return (x > 0) * np.exp( - (x - mu)**2 / (2 * sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombination model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_eta(t, tr, eta):\n",
    "    params = [tr, eta]\n",
    "    y0 = [(1- eta), 1]\n",
    "    # print(eta)\n",
    "    \n",
    "    # Define the differential equations to solve\n",
    "    def f(y, t, params):\n",
    "        n_minus, n_plus = y\n",
    "        tau, eta  = params\n",
    "        alpha = 1/tau\n",
    "        derivs = [- alpha * n_minus * n_plus, - alpha * n_minus * n_plus]\n",
    "        return derivs\n",
    "    \n",
    "    psoln = odeint(f, y0, t, args=(params,))\n",
    "    # Return only the electron number...\n",
    "    return psoln\n",
    "\n",
    "def n_product(t, tr, eta):\n",
    "    psoln = n_eta(t, tr, eta)\n",
    "    n_e = psoln[:, 0]\n",
    "    n_holes = psoln[:, 1]\n",
    "    return 1/ tr * n_e * n_holes\n",
    "\n",
    "\n",
    "def Ir3(t, tau, tr, eta, tmax, nsteps):\n",
    "    t_fine = np.linspace(0, tmax, nsteps)\n",
    "    pdf = np.exp(-t_fine/tau) / tau * np.cumsum(n_product(t_fine, tr, eta) * np.exp(t_fine/tau))\n",
    "    return np.interp(t, t_fine, pdf)\n",
    "\n",
    "def Ir3_cdf(t, tau, tr, eta, tmax, nsteps):\n",
    "    pdf = Ir3(t, tau, tr, eta, tmax, nsteps)\n",
    "    return np.cumsum(pdf) / sum(pdf)\n",
    "\n",
    "def Ir3_cdf_inv(t, tau, tr, eta, tmax, nsteps):\n",
    "    cdf = Ir3_cdf(t, tau, tr, eta, tmax, nsteps)\n",
    "    # 0 in the cdf means at time zero \n",
    "    return scipy.interpolate.interp1d(cdf, t, fill_value=(0, np.inf), bounds_error=False)\n",
    "\n",
    "def simulate_recombination_times2(nphotons, tau, tr, eta, tmax, nsteps):\n",
    "    t = np.linspace(0, tmax, nsteps)\n",
    "    return Ir3_cdf_inv(t, tau, tr, eta, tmax, nsteps)(np.random.rand(nphotons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "def split_s1_groups(x, n_x, areas, channels, **params):\n",
    "    \"\"\"Splits x into groups with uniform (s1_min, s1_max) elements, then return matrix of histograms per group.\n",
    "    Returns: integer array (n_x, n_groups)\n",
    "    n_x: number of possible values in x. Assumed to be from 0 ... n_x - 1\n",
    "    s1_min: minimum S1 number of hits\n",
    "    s1_max: maximum S1 number of hits\n",
    "    \"\"\"\n",
    "    params = get_params(params)\n",
    "    # We want to exhaust the indices x. Simulate a generous amount of S1 sizes\n",
    "    n_s1_est = int(1.5 * 2 * len(x) / (params['s1_min'] + params['s1_max']))\n",
    "    \n",
    "    if params['s1_sample'] == 'data' and 'xams_data' not in globals():\n",
    "        print('Warning: data-derived s1 area distribution not possible, reverting to uniform...')\n",
    "        params['s1_sample'] = 'uniform'\n",
    "    if params['s1_sample'] == 'uniform':\n",
    "        pe_per_s1 = (params['s1_max'] - params['s1_min']) * np.random.random(size=n_s1_est) + params['s1_min']\n",
    "    elif params['s1_sample'] == 'data':\n",
    "        # Take S1 from the data sample\n",
    "        s1s_data = xams_data[params['dset']]['s1']\n",
    "        s1s_data = s1s_data[(s1s_data >= params['s1_min']) & (s1s_data < params['s1_max'])]\n",
    "        pe_per_s1  = np.random.choice(s1s_data, size=n_s1_est)\n",
    "    else:\n",
    "        raise ValueError('Configuration not understood, got this: ', params['s1_sample'])\n",
    "    # These are two arrays for the two channels\n",
    "    # i.e. these will later yield the top and bottom waveform\n",
    "    result0 = np.zeros((n_x, n_s1_est), dtype=float)\n",
    "    result1 = np.zeros((n_x, n_s1_est), dtype=float)\n",
    "    s1_i = _split_s1_groups(x, pe_per_s1, result0, result1, areas, channels)\n",
    "    return result0[:,:s1_i - 1], result1[:,:s1_i - 1]\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _split_s1_groups(x, pe_per_s1, result0, result1, areas, channels):\n",
    "    # One of these days, I'm going to cut you into little pieces\n",
    "    s1_i = 0\n",
    "    for photon_i, (i, ch) in enumerate(zip(x, channels)):\n",
    "        if pe_per_s1[s1_i] < 0:\n",
    "            s1_i += 1\n",
    "            continue\n",
    "        if ch == 0:\n",
    "            result0[i, s1_i] += areas[photon_i]\n",
    "        if ch == 1:\n",
    "            result1[i, s1_i] += areas[photon_i]\n",
    "        pe_per_s1[s1_i] -= areas[photon_i]\n",
    "    return s1_i \n",
    "\n",
    "\n",
    "def shift(x, n):\n",
    "    \"\"\"Shift the array x n samples to the right, adding zeros to the left.\"\"\"\n",
    "    if n > 0:\n",
    "        return np.pad(x, (n, 0), mode='constant')[:len(x)]\n",
    "    else:\n",
    "        return np.pad(x, (0, -n), mode='constant')[-len(x):]\n",
    "\n",
    "\n",
    "\n",
    "def simulate_s1_pulse(**params):\n",
    "    # n_photons=int(2e5), \n",
    "    \"\"\"Return (wv_matrix, time_matrix, t_shift vector) for simulated S1s, consisting of n_photons in total\n",
    "    \"\"\"\n",
    "    params = get_params(params)\n",
    "    n_photons = params['n_photons']\n",
    "\n",
    "    ##\n",
    "    # Make matrix (n_samples, n_waveforms) of pulse waveforms with various shifts\n",
    "    ##\n",
    "    wv_matrix_list = []\n",
    "    for ch in [0, 1]:\n",
    "        # This is a matrix filled with waveforms, ordered by their SHIFT.\n",
    "        # So, these are all just model waveforms and will be selected later\n",
    "        y = spe_ys_fine[ch]\n",
    "        i_noshift = np.searchsorted(spe_t_edges_fine, [0])[0]    # Index corresponding to no shift in the waveform\n",
    "        wv_matrix = np.vstack([rebin(shift_samples(y, i - i_noshift), int(dt / dt_fine))\n",
    "                           for i in range(len(spe_ts_fine))]).T \n",
    "        wv_matrix_list.append(wv_matrix)\n",
    "    \n",
    "    # Include SPE errors\n",
    "    # Assumed the same for both channels...\n",
    "    y = spe_dys_fine\n",
    "    i_noshift = np.searchsorted(spe_t_edges_fine, [0])[0]    # Index corresponding to no shift in the waveform\n",
    "    wv_err_matrix = np.vstack([rebin(shift_samples(y, i - i_noshift), int(dt / dt_fine))\n",
    "                           for i in range(len(spe_ts_fine))]).T \n",
    "    \n",
    "    ##\n",
    "    # Simulate S1 pulse times, convert to index\n",
    "    ##\n",
    "\n",
    "    # Channel selector\n",
    "    n_top = np.random.binomial(n=n_photons, p=params['aft']) # Number of photons happening in top array\n",
    "    # First all the top channels, then all the bottom channels (ch1)\n",
    "    channels = np.concatenate([np.zeros(n_top, dtype=int), np.ones(n_photons - n_top, dtype=int)])\n",
    "    areas = np.concatenate([area_sample(n_top, gain_params, channel=0), \n",
    "                            area_sample(n_photons - n_top, gain_params, channel=1)])\n",
    "    # Shuffle the two lists the exact same way\n",
    "    channels, areas = unison_shuffled_copies(channels, areas)\n",
    "    \n",
    "    # Time is distributed according to exponential distribution\n",
    "    # This is the TRUE time of all the photons generated, assuming time=0  is the time of the interaction\n",
    "    times = np.zeros(n_photons)\n",
    "\n",
    "    if params['s1_model'] == 'two_exp':\n",
    "        n_singlets = np.random.binomial(n=n_photons, p=params['fs']) # We randomly select if the photon came from a \n",
    "                                                                     # singlet or triplet decay\n",
    "        times += np.concatenate([\n",
    "            np.random.exponential(params['t1'], n_singlets),\n",
    "            np.random.exponential(params['t3'], n_photons - n_singlets)\n",
    "        ])\n",
    "    elif params['s1_model'] == 'recombination':\n",
    "        n_recombination = np.random.binomial(n=n_photons, p=params['f_r'])\n",
    "        n_recombination_singlets = np.random.binomial(n=n_recombination, p=params['fs_r'])\n",
    "        n_recombination_triplets = n_recombination - n_recombination_singlets\n",
    "        n_direct = n_photons - n_recombination\n",
    "        n_direct_singlets = np.random.binomial(n=n_direct, p=params['fs'])\n",
    "        n_direct_triplets = n_direct - n_direct_singlets\n",
    "        assert (n_recombination_singlets + n_recombination_triplets + n_direct_singlets + n_direct_triplets == n_photons)        \n",
    "        times += np.concatenate([\n",
    "            np.random.exponential(params['t1'], n_direct_singlets),\n",
    "            np.random.exponential(params['t3'], n_direct_triplets),\n",
    "            simulate_recombination_times2(n_recombination_singlets, params['t1'], params['tr'], params['eta'],\n",
    "                                          250, 1251), \n",
    "            simulate_recombination_times2(n_recombination_triplets, params['t3'], params['tr'], params['eta'],\n",
    "                                          250, 1251), \n",
    "        ])    \n",
    "    else:\n",
    "        raise ValueError('S1 model type not understood, got this: %s' % params['s1_model'])\n",
    "\n",
    "    # Since `times` is now sorted in (singlet, triplet), shuffle them\n",
    "    np.random.shuffle(times)\n",
    "    \n",
    "    # Here we start taking into account detector physics: the transit time spread (simulated as normal dist.)\n",
    "    times += np.random.normal(0, params['tts'], size=n_photons)\n",
    "    \n",
    "    # Find the bin that the photon would be in if it were sampled.\n",
    "    # Now, we delete all the photons that are outside of the bin range and re-match to the bin centers\n",
    "    # (Check the searchsorted documentation)\n",
    "    indices = np.searchsorted(spe_t_edges_fine, times)\n",
    "    indices = indices[~((indices == 0) | (indices == len(spe_t_edges_fine)))] - 1\n",
    "\n",
    "    # This is the new amount of photons simulated\n",
    "    if len(indices) < n_photons:\n",
    "        # print('Warning: I just threw away %d photons...' % (n_photons - len(indices)))\n",
    "        n_photons = len(indices)\n",
    "    \n",
    "    ##\n",
    "    # Build instruction matrix, simulate waveforms\n",
    "    ##\n",
    "    # So far, we've just been simulating a bunch of photons (very many).\n",
    "    # We are now going to split this into S1s: the split will be made at a random point between s1_min and s1_max.\n",
    "    # `index_matrix` is a matrix split into groups forming S1s. \n",
    "    # We've got two for the two channels    \n",
    "    index_matrix0, index_matrix1 = split_s1_groups(indices, len(spe_t_edges_fine) - 1, areas, channels, **params)\n",
    "\n",
    "    # Now, index_matrix[:, 0] contains a list of number of entries for the shift for each timestamp in bin\n",
    "    n_s1 = index_matrix0.shape[1]\n",
    "    \n",
    "    # Remember that wv_matrix is a matrix of waveforms, each element at position i of which is shifted i samples\n",
    "    s1_waveforms = np.dot(wv_matrix_list[0], index_matrix0) + np.dot(wv_matrix_list[1], index_matrix1)\n",
    "    s1_waveforms_error = np.dot(wv_err_matrix, index_matrix1)\n",
    "    ##\n",
    "    # Alignment based on maximum sample, compute average pulse\n",
    "    ##\n",
    "    time_matrix, t_shift = aligned_time_matrix(spe_ts, s1_waveforms)    \n",
    "    return s1_waveforms, s1_waveforms_error, time_matrix, t_shift\n",
    "\n",
    "def aligned_time_matrix(ts, wv_matrix, mode = '10p'):\n",
    "    \"\"\"Return time matrix that would align waveforms im wv_matrix\"\"\"\n",
    "    n_s1 = wv_matrix.shape[1]\n",
    "\n",
    "    fraction_reached = np.cumsum(wv_matrix, axis=0) / np.sum(wv_matrix, axis=0)\n",
    "    # Get the sample where 10% is reached by taking the sample closest to the 10% point\n",
    "    # This is as good as you can get without introducing fractional samples (which may be an improvement)\n",
    "    # TODO get interpolation in here\n",
    "    distance_to_10p_point = np.abs(fraction_reached - 0.1)\n",
    "    t_shift = ts[np.argmin(distance_to_10p_point, axis=0)]\n",
    "    \n",
    "    time_matrix = np.repeat(ts, n_s1).reshape(wv_matrix.shape)\n",
    "    time_matrix -= t_shift[np.newaxis,:]\n",
    "    return time_matrix, t_shift\n",
    "\n",
    "def average_pulse(time_matrix, wv_matrix, **params):\n",
    "    \"\"\"Return average pulse, given time and waveform matrices\"\"\"\n",
    "    params = get_params(params)\n",
    "    h, _     = np.histogram(time_matrix, bins=spe_t_edges, weights=wv_matrix)\n",
    "    # normalize within time range\n",
    "    sel  = (spe_ts >= params['t_min']) & (spe_ts < params['t_max'])\n",
    "    h = h / np.sum(h[sel])\n",
    "    return h\n",
    "\n",
    "def average_pulse_and_error(time_matrix, wv_matrix, wv_err_matrix, **params):\n",
    "    \"\"\"Return average pulse and error, given time and waveform matrices\"\"\"\n",
    "    params = get_params(params)\n",
    "    h, _     = np.histogram(time_matrix, bins=spe_t_edges, weights=wv_matrix)\n",
    "    h_err, _ = np.histogram(time_matrix, bins=spe_t_edges, weights=wv_err_matrix)\n",
    "    sel  = (spe_ts >= params['t_min']) & (spe_ts < params['t_max'])\n",
    "    h_err = h_err / np.sum(h[sel])\n",
    "    h = h / np.sum(h[sel])\n",
    "    return h, h_err\n",
    "\n",
    "def s1_average_pulse_model(*args, **kwargs):\n",
    "    wv_matrix, wv_err_matrix, time_matrix, _ = simulate_s1_pulse(*args, **kwargs)\n",
    "    model, error = average_pulse_and_error(time_matrix, wv_matrix, wv_err_matrix,**kwargs)\n",
    "    return model, error\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    '''Stack overflow to the rescue'''\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "def s1_models_error(*args, shifts=None, mode='std', **kwargs):\n",
    "    '''\n",
    "    Gives the systematic error, split into two parts.\n",
    "    Returns `model`, `sigma_syst`, `spe_error`\n",
    "    '''\n",
    "    if kwargs.get('neglect_systematic', default_params['neglect_systematic']):\n",
    "        model, error = s1_average_pulse_model(*args, **kwargs)\n",
    "        return model, np.zeros(len(model)), error\n",
    "    \n",
    "    if shifts is None:\n",
    "        # Default uncertainty: in pulse model\n",
    "        shifts = dict(aft=0.12)\n",
    "    \n",
    "    # Allow specifying a single +- amplitude of variation\n",
    "    for p, shift_values in shifts.items():\n",
    "        if isinstance(shift_values, (float, int)):\n",
    "            shifts[p] = kwargs.get(p, default_params[p]) + np.array([-1, 0, 1]) * shift_values\n",
    "    \n",
    "\n",
    "    shift_pars = sorted(shifts.keys())\n",
    "    shift_values = [shifts[k] for k in shift_pars]\n",
    "    # shift_value_combs is a list of paramters that will be tried to compute the average pulse.\n",
    "    # Contains all combintations: (+, 0, -) for all the parameters. ((3n)^2 for n number of parameters.)\n",
    "    shift_value_combs = list(itertools.product(*shift_values))\n",
    "    noshift_comb = tuple([(kwargs.get(p, default_params[p])) for p, shift_values in shifts.items()])\n",
    "    noshift_index = int((len(shift_value_combs) -1) /2)\n",
    "    # Check if we have the right index\n",
    "    \n",
    "    for i, comb in enumerate(shift_value_combs):\n",
    "        if np.all([np.isclose(a,b) for a, b in zip(comb, noshift_comb)]):\n",
    "            noshift_index = i\n",
    "                \n",
    "    \n",
    "    alt_models = []\n",
    "    for vs in shift_value_combs:\n",
    "        kw = dict()\n",
    "        kw.update(kwargs)\n",
    "        for i, p in enumerate(shift_pars):\n",
    "            kw[p] = vs[i]        \n",
    "        alt_models.append(s1_average_pulse_model(*args, **kw))\n",
    "    # The alt_models list will now contain (model, error) for each parameter combination.\n",
    "    # Here, split them in twain\n",
    "    alt_errs = [alt_model[1] for alt_model in alt_models]\n",
    "    alt_models = [alt_model[0] for alt_model in alt_models]\n",
    "    \n",
    "    alt_models = np.vstack(alt_models)\n",
    "    spe_err = np.vstack(alt_errs)\n",
    "    model = alt_models[noshift_index]\n",
    "    \n",
    "    spe_err = alt_errs[noshift_index]\n",
    "    sigma_sys = np.sqrt(np.std(alt_models, axis=0)**2)\n",
    "    #minus = base_model - sigma_sys\n",
    "    #plus = base_model + sigma_sys\n",
    "\n",
    "    return model, sigma_sys, spe_err\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the S1 data for three (highfield) datasets: NR, ER and BG_NR. We store it in the form of a dict (keys: er, nr, bg_nr). Each dict item is an array containing the waveforms (per row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading XAMS data from pickles...\n"
     ]
    }
   ],
   "source": [
    "print('Reading XAMS data from pickles...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data in 5.36 seconds\n"
     ]
    }
   ],
   "source": [
    "before = time.time()\n",
    "xams_data = dict()\n",
    "xams_data['nr'], xams_data['er'], xams_data['bg_nr'] = pickle.load(open(os.path.join(pickle_dir, \n",
    "                                                                                     'highfield_dataframes_new.pickle'),'rb'))\n",
    "xams_data['er_0'] = pickle.load(open(os.path.join(pickle_dir, 'zerofield_dataframes_temp.pickle'), 'rb'))\n",
    "xams_data['nr_l'], xams_data['er_l'] = pickle.load(open(os.path.join(pickle_dir, 'lowfield_dataframes.pickle'), 'rb'))\n",
    "xams_s1s = dict()\n",
    "# Get pulse waveforms to matrix rather than object column\n",
    "# Also cut off a bit at the end to account for shorter waveform in high resolution pulse\n",
    "max_index = len(spe_ts)\n",
    "for k, d in xams_data.items():\n",
    "    xams_s1s[k] = np.array([x[:max_index] for x in d['s1_pulse']])\n",
    "    del d['s1_pulse']\n",
    "after = time.time()\n",
    "print('Read data in %.2f seconds' % (after - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now use 0.28 +- 0.12 as uncertainty of the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment, averaging, bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def real_s1_wv(**params):\n",
    "    \"\"\"Return average S1 waveform, number of S1s it was constructed from\"\"\"\n",
    "    params = get_params(params)\n",
    "    \n",
    "    areas = xams_data[params['dset']]['s1'].values\n",
    "    mask = (params['s1_min'] < areas) & (areas < params['s1_max'])\n",
    "\n",
    "    n_data_s1s = mask.sum()\n",
    "    wvs = xams_s1s[params['dset']][mask].T\n",
    "    tmat, _ = aligned_time_matrix(spe_ts, wvs)\n",
    "    real_s1_avg =  average_pulse(tmat, wvs, **params)\n",
    "    \n",
    "    return real_s1_avg, n_data_s1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_s1_wv_sigma(**params):\n",
    "    \"\"\"Take data S1s, bootstrap sample, then check what the variance is\"\"\"\n",
    "    params = get_params(params)\n",
    "    if params['stored_stat']:\n",
    "        if params['stored_stat_value'] is not None:\n",
    "            return params['stored_stat_value']\n",
    "        else:\n",
    "            print('Warning: stored_stat not found, will revert to computing again...')\n",
    "    bootstrap_trials = params['bootstrap_trials']\n",
    "    time_matrix, wv_matrix = real_s1_wv_matrix(**params)\n",
    "    n_s1s = wv_matrix.shape[1]\n",
    "    waveform_templates = np.zeros((len(spe_ts), bootstrap_trials))\n",
    "\n",
    "    for i in range(bootstrap_trials):\n",
    "        new_indices = np.random.randint(n_s1s, size=n_s1s)\n",
    "\n",
    "        waveform_templates[:, i] = average_pulse(time_matrix[:, new_indices], \n",
    "                                                 wv_matrix[:, new_indices], **params)\n",
    "        \n",
    "    \n",
    "    return np.std(waveform_templates, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_s1_wv_matrix(**params):\n",
    "    \"\"\"Return the aligned time matrix and waveform matrix.\"\"\"\n",
    "    params = get_params(params)\n",
    "    \n",
    "    areas = xams_data[params['dset']]['s1'].values\n",
    "    mask = (params['s1_min'] < areas) & (areas < params['s1_max'])\n",
    "\n",
    "    n_data_s1s = mask.sum()\n",
    "    wvs = xams_s1s[params['dset']][mask].T\n",
    "    tmat, _ = aligned_time_matrix(spe_ts, wvs)\n",
    "        \n",
    "    return tmat, wvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-data comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigma and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def residuals(ydata, model, syst_err, spe_err, **params):\n",
    "    params = get_params(params)\n",
    "    sigma = get_sigma(model, syst_err, spe_err, **params)\n",
    "    if 0. in sigma:\n",
    "        zero_positions = np.where(sigma == 0)\n",
    "        print('Warning: found zero in error array at positions: ', zero_positions)\n",
    "        print('Replacing with infinite error instead...')\n",
    "        for pos in zero_positions:\n",
    "            sigma[pos] = np.inf\n",
    "    return (ydata - model) / sigma\n",
    "\n",
    "def get_sigma(model, syst_err, spe_err, **params):\n",
    "    '''\n",
    "    Combine the syst. errors with stat error.\n",
    "    '''\n",
    "    params = get_params(params)\n",
    "    if params['neglect_statistical']:\n",
    "        print('Neglecting statistical error...')\n",
    "        return np.sqrt(syst_err**2 + spe_err**2)\n",
    "    sigma_stat = real_s1_wv_sigma(**params)\n",
    "    sigma = np.sqrt(syst_err**2 + spe_err**2 + sigma_stat**2 + params['error_offset']**2)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparison_plot(ydata, model, syst_err, spe_err, log=False, **params):\n",
    "    params = get_params(params)\n",
    "    sigmas = get_sigma(model, syst_err, spe_err, **params)\n",
    "\n",
    "    # large subplot\n",
    "    ax2 = plt.subplot2grid((3,1), (2,0))\n",
    "    ax1 = plt.subplot2grid((3,1), (0,0), rowspan=2, sharex=ax2)\n",
    "\n",
    "    #f, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    plt.sca(ax1)\n",
    "    # plt.fill_between(spe_ts, minus, plus, alpha=0.5, linewidth=0, step='mid')\n",
    "    plt.fill_between(spe_ts, model - sigmas, model + sigmas,\n",
    "                     alpha=0.5, linewidth=0, step='mid')\n",
    "    plt.plot(spe_ts, model, linestyle='steps-mid', label='Model')\n",
    "    plt.plot(spe_ts, ydata, marker='.', linestyle='', markersize=3, c='k', label='Observed')\n",
    "\n",
    "    plt.grid(alpha=0.1, linestyle='-', which='both')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.ylabel(\"Fraction of amplitude\")\n",
    "    plt.axhline(0, c='k', alpha=0.5)\n",
    "    leg = plt.legend(loc='upper right', numpoints=1)\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(1e-5, 1e-1)\n",
    "    else:\n",
    "        plt.ylim(0, None)\n",
    "\n",
    "    # Add residuals\n",
    "    plt.sca(ax2)\n",
    "    plt.subplot2grid((3,1), (2,0), sharex=ax1)\n",
    "    plt.xlim(params['t_min'], params['t_max'])\n",
    "\n",
    "    res = (ydata - model) / sigmas # residuals(ydata, minus, base, plus, **params)\n",
    "    \n",
    "    plt.plot(spe_ts, res,\n",
    "             linestyle='--', marker='x', c='k', markersize=3)\n",
    "    # plt.ylim(-3, 3)\n",
    "    plt.grid(which='both', linestyle='-', alpha=0.1)\n",
    "    plt.axhline(0, c='k', alpha=0.5)\n",
    "\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.xlabel(\"Time since alignment point\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(0,0,1,1,0,0)\n",
    "\n",
    "def comparison_plot_2(ydata, model, syst_err, spe_err, plot_residual = True, **params):\n",
    "    params = get_params(params)\n",
    "    sigmas = get_sigma(model, syst_err, spe_err, **params)\n",
    "    res = (ydata - model) / sigmas \n",
    "\n",
    "    plt.fill_between(spe_ts, model - sigmas, model + sigmas,\n",
    "                     alpha=0.5, linewidth=0, step='mid')\n",
    "    plt.plot(spe_ts, model, linestyle='steps-mid', label='Model')\n",
    "    plt.plot(spe_ts, ydata, marker='.', linestyle='', markersize=3, c='k', label='Observed')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(2e-5, 1e-1)\n",
    "    plt.ylabel(\"Fraction of amplitude\")\n",
    "    plt.xlabel('Time (ns)')\n",
    "    for _l in (params['t_min'], params['t_max']):\n",
    "        plt.axvline(_l, ls='dotted', color='black')\n",
    "    if plot_residual:\n",
    "        plt.twinx()\n",
    "        plt.plot(spe_ts, np.abs(res), color='red')\n",
    "        plt.ylabel('Residual / error')\n",
    "        plt.ylim(0)\n",
    "    plt.xlim(params['t_min'] - 20, params['t_max'] + 50)\n",
    "    \n",
    "    res = res[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])]\n",
    "    chi2 = sum(res**2) / len(spe_ts[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])])\n",
    "    print('chi2 = %f' % chi2)\n",
    "    \n",
    "def plot_model(plot_type = 0, figname = None, plot_residual = True, **params):\n",
    "    params = get_params(params)\n",
    "    print(params)\n",
    "    ydata, _ = real_s1_wv(**params)\n",
    "    model, syst_err, spe_err = s1_models_error(**params)\n",
    "    if plot_type == 0 or plot_type == 1:\n",
    "        comparison_plot(ydata, model, syst_err, spe_err, **params)\n",
    "        if figname is not None:\n",
    "            plt.savefig('figs/' + figname + '_1.png', bbox_inches = 'tight', dpi = 400)\n",
    "        plt.show()\n",
    "    if plot_type == 0 or plot_type == 2:\n",
    "        comparison_plot_2(ydata, model, syst_err, spe_err, plot_residual = plot_residual, **params)\n",
    "        if figname is not None:\n",
    "            plt.savefig('figs/' + figname + '_2.png', bbox_inches = 'tight', dpi = 400)\n",
    "\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gof(verbose=True, **params):\n",
    "    '''\n",
    "    Get the value to minimize given the parameters\n",
    "    '''\n",
    "    params = get_params(params)\n",
    "    # Do not allow unphysical values\n",
    "    if params['t1'] < 0 or params['t3'] < 0 or not (0 <= params['fs'] <= 1):\n",
    "        result = float('inf')\n",
    "    else:\n",
    "        ydata, _ = real_s1_wv(**params)\n",
    "        model, syst_err, spe_err = s1_models_error(**params)\n",
    "        res = residuals(ydata, model, syst_err, spe_err, **params)\n",
    "        assert len(res) == len(spe_ts)\n",
    "        res = res[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])]\n",
    "        # Computed chi2 over NDF\n",
    "        result = 1/len(res) * np.sum(res**2)\n",
    "    if verbose:\n",
    "        print('gof={gof}, fs={fs}, t1={t1}, t3={t3}, tts={tts}'.format(gof=result, **params))\n",
    "    return result\n",
    "\n",
    "def gof_manual(ydata, model, syst_err, spe_err, verbose=True, **params):\n",
    "    params = get_params(params)\n",
    "    res = residuals(ydata, model, syst_err, spe_err, **params)\n",
    "    assert len(res) == len(spe_ts)\n",
    "    res = res[(spe_ts >= params['t_min']) & (spe_ts < params['t_max'])]\n",
    "    # Computed chi2 over NDF\n",
    "    result = 1/len(res) * np.sum(res**2)\n",
    "    if verbose: print('gof={gof}, fs={fs}, t1={t1}, t3={t3}, tts={tts}'.format(gof=result, **params))\n",
    "    return result\n",
    "\n",
    "# Depricated as of November 2017, scheduled for delete\n",
    "# def gof_repeat(iterations, verbose=True, mode = 'chi2_ndf', metamode = 'median', **params):\n",
    "#     params = get_params(params)\n",
    "#     if params['t1'] < 0 or params['t3'] < 0 or not (0 <= params['fs'] <= 1):\n",
    "#         result = float('inf')\n",
    "#     else:\n",
    "#         gofs = np.array([gof(verbose=False, **params) for _ in range(iterations)])\n",
    "#         if metamode == 'median':\n",
    "#             result = np.median(gofs)\n",
    "#         elif metamode == 'mean':\n",
    "#             result = np.mean(gofs)\n",
    "#     if verbose:\n",
    "#         print('gof={gof}, fs={fs}, t1={t1}, t3={t3}, tts={tts}'.format(gof=result, **params))\n",
    "#     return result    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Depricated as of November 2017, scheduled for delete\n",
    "# def gof_simultaneous(fs_er, fs_nr, verbose=True, mode='mean', **params):\n",
    "#     params = get_params(params)\n",
    "#     params_er = deepcopy(params)\n",
    "#     params_nr = deepcopy(params)\n",
    "#     params_er['dset'] = 'er'\n",
    "#     params_nr['dset'] = 'nr'\n",
    "#     params_er['fs'] = fs_er\n",
    "#     params_nr['fs'] = fs_nr\n",
    "#     gof_er = gof(verbose=False, mode=mode, **params_er)\n",
    "#     gof_nr = gof(verbose=False, mode=mode, **params_nr)\n",
    "#     if verbose:\n",
    "#         print('gof_er={gof_er}, gof_nr={gof_nr}, fs_er={fs_er}, fs_nr={fs_nr} t1={t1}, t3={t3}, tts={tts}'.format(\n",
    "#             gof_er=gof_er, gof_nr=gof_nr, fs_er = params_er['fs'], fs_nr = params_nr['fs'], **params))    \n",
    "#     return gof_er + gof_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "def minimize_it(param_names, starting_values, direc, **params):\n",
    "    optresult = optimize.minimize(\n",
    "        lambda x: gof(**merge_two_dicts(params, {par : x[i] for i, par in enumerate(param_names)})),\n",
    "        starting_values,\n",
    "        options=dict(maxfev=1000, direc=direc),\n",
    "        method='Powell',\n",
    "    )\n",
    "    return optresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Depricated as of November 2017, scheduled for delete\n",
    "# def check_combined_model(Linf, plot=False, plot_type = 1, **p):\n",
    "#     # Extract the recombination fractions from Linf\n",
    "#     if Linf > 143:\n",
    "#         return np.inf\n",
    "#     dsets_er = ['er_0', 'er_l', 'er']\n",
    "#     L = [200, 168, 143] # light yield for 0, 100 and 500 field\n",
    "#     frs = [l / Linf -1 for l in L]\n",
    "#     gofs = [] \n",
    "#     for dset, fr in zip(dsets_er, frs):\n",
    "#         p['dset'] = dset\n",
    "#         p['f_r'] = fr\n",
    "#         if plot:\n",
    "#             plot_model(plot_type=plot_type, **p)\n",
    "#         gofs.append(gof(**p))\n",
    "#     print(gofs)\n",
    "    \n",
    "#     return sum(gofs) #  + sum(prior_thingy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spe model error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dy(t, t0, tau, a, const, cutoff):\n",
    "    t = t - t0\n",
    "    return (t >= cutoff) * (a * np.exp(-t/tau) + const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spe_dys_fine = dy(spe_ts_fine, 0, 70, 0.00002, 0.0000, 15)\n",
    "# for i, ys in enumerate(spe_ys_fine):\n",
    "#     plt.plot(spe_ts_fine, ys)\n",
    "#     plt.ylim(-0.0002, 0.0002)\n",
    "#     plt.plot(spe_ts_fine, spe_dys_fine)\n",
    "#     plt.axhline(0, ls='--', lw=1, c='black')\n",
    "#     plt.xlim(-20, 200)\n",
    "#     plt.title('Channel %d' % i)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automatic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_settings_dicts(scan_params, lower_bounds, upper_bounds, step_sizes, block_size, **p):\n",
    "    '''\n",
    "    Build a number of settings dicts in an array. These settings scan over the bounds values.\n",
    "    '''\n",
    "    x = []\n",
    "    nsteps = [int((u - l)/s) + 1 for u, l, s in zip(upper_bounds, lower_bounds, step_sizes)]\n",
    "    for param, l, u, ns in zip(scan_params, lower_bounds, upper_bounds, nsteps):\n",
    "        x.append(np.linspace(l, u, ns))\n",
    "    n_sim  = int(np.product(nsteps))\n",
    "    x = np.meshgrid(*x)\n",
    "    par_vals = [_x.flatten() for _x in x]\n",
    "    dicts = []\n",
    "    for i in range(n_sim):\n",
    "        for j, par in enumerate(scan_params):\n",
    "            p[par] = par_vals[j][i]\n",
    "        dicts.append(deepcopy(p))\n",
    "    dicts = np.array(dicts)\n",
    "    print('Loaded %d settings.' % len(dicts))\n",
    "    if len(dicts) <= block_size:\n",
    "        return dicts\n",
    "    else:\n",
    "        n_blocks = int(len(dicts) / block_size)\n",
    "        return np.array_split(dicts, n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_settings(dicts):\n",
    "    for p in dicts:\n",
    "        p['chi2'] = gof(**p)\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_settings_pickles(x, dirname, base_name):\n",
    "    fns = []\n",
    "    for i, _x in enumerate(x):\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        fn = os.path.join(dirname, base_name + '_%03d.pickle' % i)\n",
    "        fns.append(fn)\n",
    "        with open(fn, 'wb') as f:\n",
    "            pickle.dump(_x, f)\n",
    "    print('Dumping done.')\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_pickle_read(fn):\n",
    "    with open(fn, 'rb') as f:\n",
    "        dicts = pickle.load(f)\n",
    "    return dicts\n",
    "\n",
    "def single_pickle_dump(fn, dicts):\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(dicts, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def process_the_pickle(fn):\n",
    "#     with open(fn, 'rb') as f:\n",
    "#         dicts = pickle.load(f)\n",
    "#     dicts = process_settings(dicts)\n",
    "#     with open(fn, 'wb') as f:\n",
    "#         pickle.dump(dicts, f)\n",
    "#     print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
